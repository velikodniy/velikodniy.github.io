---
layout: post
title: Вейвлет-сжатие на пальцах
---

Когда-то давно, ещё в феврале 2013 года я опубликовал две статьи на
Хабре, посвящённые вейвлет-сжатию. Моей целью было максимально просто
и доступно, на уровне школы или первого курса рассказать о
вейвлетах. Во второй, [практической
части](https://habr.com/ru/post/169615/) многое нужно уже исправить и
переработать. А вот [первая часть](https://habr.com/ru/post/168517/)
всё ещё годится в качестве начального введения. Привожу её с
исправлениями и дополнениями. В частности, я обновил
иллюстрации и добавил небольшой раздел о биортогональных вейвлетах.

Введение
========

Вейвлеты сейчас на слуху. Даже неискушённые в математике люди
наверняка слышали, что с их помощью удаётся сжимать изображения и
видео сохраняя приемлемое качество. Но что же такое вейвлет? Википедия
отвечает на этот вопрос целым ворохом формул за которыми не так-то
легко увидеть суть.

Попробуем на простых примерах разобраться, откуда же вообще берутся
вейвлеты и как их можно использовать при сжатии. Предполагается, что
читатель знаком с основами линейной алгебры, не боится слов вектор и
матрица, а также умеет их перемножать.

Сжатие изображений
==================

Упрощённо, изображение представляют собой таблицу, в ячейках которой
хранятся цвета каждого пикселя. Если мы работаем с чёрно-белым (или,
точнее, серым) изображением, то вместо цвета в ячейки помещают
значения яркости из отрезка $[0, 1]$. При этом 0 соответствует чёрному
цвету, 1 — белому. Но с дробями работать неудобно, поэтому часто
значения яркости берут целыми из диапазона от 0 до 255. Тогда каждое
значение будет занимать ровно 1 байт.

Даже небольшие изображения требуют много памяти для хранения. Так,
если мы кодируем яркость каждого пикселя одним байтом, то изображение
одного кадра формата FullHD (1920×1080) займёт почти два
мегабайта. Представьте, сколько памяти потребуется для хранения
полуторачасового фильма!

Поэтому изображения стремятся сжать. То есть закодировать таким
образом, чтобы памяти для хранения требовалось меньше. А во время
просмотра мы декодируем записанные в память данные и получаем исходный
кадр. Но это лишь в идеале.

Существует много алгоритмов сжатия данных. О их количестве можно
судить по форматам, поддерживаемым современными архиваторами: ZIP, 7zip,
RAR, gzip, bzip2 и так далее. Неудивительно, что благодаря
активной работе учёных и программистов в настоящее время степень
сжатия данных вплотную подошла к теоретическому пределу.

Плохая новость в том, что для изображения этот теоретический предел не
так уж и велик. Попробуйте сохранить фотографию (особенно с большим
количеством мелких деталей) в формате PNG — размер получившегося файла
может вас расстроить.

Это происходит из-за того, что в изображениях из реального мира
(фотографиях, например) значения яркости редко бывают одинаковыми даже
у соседних пикселей. Всегда есть мельчайшие колебания, которые
неуловимы человеческим глазом, но которые алгоритм сжатия честно
пытается учесть.

Алгоритмы сжатия «любят», когда в данных есть закономерность. Лучше
всего сжимаются длинные последовательности нулей (закономерность тут
очевидна). В самом деле, вместо того, чтобы записывать в память 100
нулей, можно записать просто число 100 (конечно, с пометкой, что это
именно количество нулей). Декодирующая программа «поймёт», что имелись
в виду нули и воспроизведёт их.

Однако если в нашей последовательности в середине вдруг окажется
единица, то одним числом 100 ограничится не удастся.

Но зачем кодировать абсолютно все детали? Ведь когда мы смотрим на
фотографию, нам важен общий рисунок, а незначительные колебания
яркости мы и не заметим. А значит, при кодировании мы можем немного
изменить изображение так, чтобы оно хорошо кодировалось. При этом
степень сжатия сразу вырастет. Правда, декодированное изображение
будет незначительно отличаться от исходного, но кто заметит?

Преобразование Хаара
====================

Итак, наша цель — преобразовать изображение так, чтобы оно хорошо
сжималось классическими алгоритмами. Подумаем, как нужно изменить его,
чтобы получить длинные цепочки нулей.

У «реальных» изображений, таких как фотографии, есть одна особенность
— яркость соседних пикселей обычно отличается на небольшую величину. В
самом деле, в мире редко можно увидеть резкие, контрастные перепады
яркости. А если они и есть, то занимают лишь малую часть изображения.

Давайте рассмотрим конкретный пример. В цифровой обработке изображений
часто в качестве тестового изображения используют фотографию Лены
Сёдерберг. Её часто можно увидеть в статьях.

![Лена Сёдерберг]({{ site.url }}/assets/2019-01-31-wavelets-theory/lena.jpg)

Чтобы пока не углубляться в детали кодирования цветных изображений,
для каждого пикселя оставим только значения яркости.

![Лена Сёдерберг (яркостная компонента)]({{ site.url }}/assets/2019-01-31-wavelets-theory/lena-grayscale.jpg)

Рассмотрим яркости первой строки пикселей.

```
154,  155,  156,  157,  157,  157,  158,  156
```

Видно, что соседние числа очень близки. Чтобы получить желаемые нули
или хотя бы что-то близкое к ним, можно закодировать отдельно первое
число, а потом рассматривать лишь отличия каждого числа от
предыдущего.

Получаем:

```
154, 1, 1, 1, 0, 0, 1, -2.
```

Уже лучше! Такой метод в самом деле используется и называется
дельта-кодированием. Но у него есть серьёзный недостаток — он
нелокальный. То есть нельзя взять кусочек последовательности и узнать,
какие именно яркости в нём закодированы без декодирования всех
значений перед этим кусочком.

Попробуем поступить иначе. Не будем пытаться сразу получить хорошую
последовательность, попробуем улучшить её хотя бы немного.

Для этого разобьём все числа на пары и найдём полусуммы и полуразности
значений в каждой из них.

```
(154, 155),   (156, 157),   (157, 157), (158, 156)
(154.5, 0.5), (156.5, 0.5), (157, 0.0), (157, -1.0)
```

Почему именно полусуммы и полуразности? А всё очень просто! Полусумма
— это среднее значение яркости пары пикселей. А полуразность несёт в
себе информацию об отличиях между значениями в паре. Очевидно, зная
полусумму $a$ и полуразность $d$ можно найти и сами значения: первое
значение в паре равно $a - d$, второе значение в паре равно $a + d$.

Это преобразование было предложено в 1909 году Альфредом Хааром и
носит его имя.

А где же сжатие?
================

Полученные числа можно перегруппировать по принципу «мухи отдельно,
котлеты отдельно», разделив полусуммы и полуразности:

```
154.5, 156.5, 157, 157;  0.5, 0.5, 0.0, -1.0.
```

Числа во второй половине последовательности как правило будут
небольшими (то, что они не целые, пусть пока не смущает). Почему так?

Как мы уже выяснили раньше, в реальных изображениях соседние пиксели
редко отличаются друг от друга значительно. Если значение одного
велико, то и другого велико. В таких случаях говорят, что соседние
пиксели коррелированы.

Запишем яркости всех пикселей построчно в виде одной длинной цепочки,
рассмотрим пары соседних пикселей и каждую пару представим на графике
точкой.

![Пары соседних пикселей]({{ site.url }}/assets/2019-01-31-wavelets-theory/pairs.jpg)

Точки выстраиваются близко к диагонали, что означает, что яркости
соседних пикселей примерно одинаковы. И так практически во всех
реальных изображениях. Верхний левый и нижний правый углы изображения
практически всегда пусты.

Ещё лучше это видно на графике, построенном по первым 2048 пикселям —
они соответствуют верхним строкам изображения, где нет резких переходов.

![Пары соседних пикселей для первых 2048 значений]({{ site.url }}/assets/2019-01-31-wavelets-theory/pairs-2048.jpg)

А теперь рассмотрим график, точками в котором будут полусуммы и
полуразности.

Это для всех пикселей.

![Полусуммы и полуразности]({{ site.url }}/assets/2019-01-31-wavelets-theory/half-sums.jpg)

А это для первых 2048 значений.

![Полусуммы и полуразности для первых 2048 значений]({{ site.url }}/assets/2019-01-31-wavelets-theory/half-sums-2048.jpg)

Видно, что полуразности находятся в более узком диапазоне значений. А
это значит, что на них можно потратить меньше одного байта (правда, с
небольшой потерей точности). Какое-никакое, а сжатие.

Применим математику!
====================

Попробуем записать математические выражения, описывающие
преобразование Хаара.

Итак, у нас была пара пикселей (вектор)
$$\begin{pmatrix} x \\ y \end{pmatrix}$$, а мы хотим
получить пару
$$\begin{pmatrix} \frac{y+x}2 \\ \frac{y-x}2 \end{pmatrix}$$.

Такое преобразование описывается матрицей

$$\begin{pmatrix}
  \phantom{-}\frac12 & \frac12 \\
            -\frac12 & \frac12
\end{pmatrix}.$$

В самом деле

$$\begin{pmatrix}
  \phantom{-}\frac12 & \frac12 \\
  -\frac12 & \frac12
\end{pmatrix}
\begin{pmatrix} x \\ y \end{pmatrix}  =
\begin{pmatrix} \frac{y+x}2 \\ \frac{y-x}2 \end{pmatrix},$$

что нам и требовалось.

Внимательный читатель наверняка заметил, что рисунки из точек на двух
последних графиках одинаковы. Разница лишь в повороте на угол в 45°.

В математике повороты, отражения и растяжения называются аффинными
преобразованиями и описываются как раз при помощи умножения матрицы на
вектор. Что мы и получили выше. То есть, преобразование Хаара — это
просто поворот точек таким образом, чтобы их можно было удобно и
компактно закодировать.

Правда, тут есть один нюанс. При аффинных преобразованиях может
меняться площадь фигуры. Не то, чтобы это было плохо, но как-то
неаккуратненько. Как известно, коэффициент изменения площади равен
определителю матрицы. Посмотрим, какой он для преобразования Хаара.

$$\det\begin{pmatrix} \phantom{-}\frac12 & \frac12 \\ -\frac12 & \frac12 \end{pmatrix} = \frac12\cdot\frac12-\left(-\frac12\cdot\frac12\right) = \frac12$$

Для того, чтобы определитель стал равен единице достаточно умножить
каждый элемент матрицы на $\sqrt2$.  На угол поворота (а значит, и на
«сжимающую способность» преобразования) умножение на константу не
повлияет.

Получаем в итоге матрицу

$$H =
\begin{pmatrix}
  \phantom{-}\frac1{\sqrt2} & \frac1{\sqrt2} \\
  -\frac1{\sqrt2} & \frac1{\sqrt2}
\end{pmatrix}.$$

А как декодировать?
===================

Как известно, если у матрицы определитель не равен нулю, то для неё
существует обратная матрица, «отменяющая» её действие. Если мы найдём
обратную матрицу для $H$, то декодирование будет заключаться просто в
умножении векторов с полусуммами и полуразностями на неё:

$$\begin{pmatrix} x \\ y \end{pmatrix} H^{-1} = \begin{pmatrix} s \\ d \end{pmatrix}.$$

Вообще говоря, поиск обратной матрицы — не такая простая задача. Но,
может, удастся как-то эту задачу упростить?

Рассмотрим поближе нашу матрицу. Она состоит из двух вектор-строк:
$$\left(\frac1{\sqrt2},\frac1{\sqrt2}\right)$$
и $$\left(-\frac1{\sqrt2},\frac1{\sqrt2}\right)$$.
Назовём их $v_1$ и $v_2$.

Они обладают интересными свойствами.

Во-первых, их длины равны 1, то есть

$$v_1 v_1^T = v_2 v_2^T = 1.$$

Здесь буква $T$ означает транспонирование — обмен строк и столбцов
местами.  Умножение вектор-строки на транспонированную вектор-строку
(то есть, вектор-столбец) — это обычное скалярное произведение.

Во-вторых, они ортогональны, то есть

$$v_1 v_2^T = v_2 v_1^T = 0.$$

Матрица, строки которой обладают указанными свойствами называется
ортогональной. Чрезвычайно важным свойством таких матриц является то,
что обратную матрицу для них можно получить простым транспонированием.

$$H^{-1} = H^T =
\begin{pmatrix} \phantom{-}\frac12 & \frac12 \\ -\frac12 & \frac12 \end{pmatrix}^T =
\begin{pmatrix} \frac12 & -\frac12 \\ \frac12 & \phantom{-}\frac12 \end{pmatrix}.$$

В справедливости этого выражения можно убедиться умножив $H$ на
обратную матрицу. На диагонали мы получим скалярные произведения
вектор-строк на самих себя, то есть 1. А вне диагоналей — скалярные
произведения вектор-строк друг на друга, то есть 0. В итоге
произведение будет равно единичной матрице

$$\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}.$$

А единичная матрица — это отсутствие преобразования, так как она не
меняет вектор.

Мы любим ортогональные матрицы!

Увеличиваем число точек
=======================

Всё сказанное хорошо работает для двух точек. Но что делать, если
точек больше?

В этом случае тоже можно описать преобразование матрицей, но большей
по размеру. Диагональ этой матрицы будет состоять из матриц H, таким
образом в векторе исходных значений будут выбираться пары, к которым
независимо будет применяться преобразование Хаара.

$$\begin{pmatrix}
\phantom{-}\frac1{\sqrt{2}} & \frac1{\sqrt{2}} &&& \\
-\frac1{\sqrt{2}} & \frac1{\sqrt{2}} &&& \\
&& \phantom{-}\frac1{\sqrt{2}} & \frac1{\sqrt{2}} & \\
&& -\frac1{\sqrt{2}} & \frac1{\sqrt{2}} & \\
&&&& \ddots
\end{pmatrix}$$

(Договоримся, что в пустых ячейках стоят нули.)

То есть, исходный вектор просто обрабатывается независимо по парам.

Фильтры
=======

Итак, когда мы знаем, как выполнять преобразование Хаара, попробуем
разобраться с тем, что же оно нам даёт.

Полученные «полусуммы» (из-за того, что делим не на 2, приходится
использовать кавычки) — это, как мы уже выяснили, средние значения в
парах пикселей. То есть, фактически, значения полусумм — это
уменьшенная копия исходного изображения! Уменьшенная потому, что
полусумм в два раза меньше, чем исходных пикселей.

Но что такое разности?

Полусуммы усредняют значения яркостей, то есть «отфильтровывают»
случайные всплески значений. Можно считать, что это некоторый
частотный фильтр, убирающий высокочастотную компоненту изображения.

Фильтр, пропускающий низкие частоты, называется фильтром низких
частот.

Аналогично, разности «выделяют» среди значений межпиксельные
«всплески» и устраняют константную составляющую. То есть, они
«отфильтровывают» низкие частоты.

Таким образом, преобразование Хаара — это пара фильтров, разделяющих
сигнал на низкочастотную и высокочастотную составляющие. Чтобы
получить исходный сигнал, нужно просто снова объединить эти
составляющие.

Что нам это даёт? Пусть у нас есть фотография-портрет. Низкочастотная
составляющая несёт в себе информацию об общей форме лица, о плавных
перепадах яркости. Высокочастотная — это шум и мелкие детали.

Обычно, когда мы смотрим на портрет, нас больше интересует
низкочастотная составляющая, а значит при сжатии часть высокочастотных
данных можно отбросить. Тем более, что, как мы выяснили, она обычно
имеет меньшие значения, а значит более компактно кодируется.

Степень сжатия можно увеличить, применяя преобразование Хаара
многократно. В самом деле, высокочастотная составляющая — это всего
лишь половина от всего набора чисел. Но что мешает применить нашу
процедуру ещё раз к низкочастотным данным? После повторного
применения, высокочастотная информация будет занимать уже 75 %.

Сжатие изображения
==================

Хоть мы пока и говорили об одномерных цепочках чисел, этот подход
хорошо применим и для двумерных данных. Чтобы выполнить двумерное
преобразование Хаара (или аналогичное ему), нужно лишь выполнить его
для каждой строки и для каждого столбца.

Для работы с цветными изображениями можно просто обрабатывать три
цветовые компоненты независимо. Для сжатия чаше используют не цветовое
пространство $RGB$ (когда цвет кодируется значениями красной, зеленой
и синей компонент), а $Y'C_BC_R$ или подобные. В них первая компонента
— яркость, а остальные две отвечают за цвет. Глаз человека более
чувствителен к изменению яркости, а значит цвет можно сжать сильнее с
бо́льшими потерями без заметного снижения качества.

Если применить этот подход к цветной фотографии, то получится 4 набора
чисел — три с высокочастотными составляющими и один с низкочастотными.

![Двумерное преобразование]({{ site.url }}/assets/2019-01-31-wavelets-theory/lena-haar-1.jpg)

Никто не мешает применить преобразование повторно к уменьшенному
низкочастотному блоку.

![Двумерное преобразование, применённое дважды]({{ site.url }}/assets/2019-01-31-wavelets-theory/lena-haar-2.jpg)

Или вовсе многократно пока позволяют размеры получающегося
низкочастотного блока.

![Двумерное преобразование, применённое многократно]({{ site.url }}/assets/2019-01-31-wavelets-theory/lena-haar.jpg)

Черные области соответствуют низкой яркости, то есть значениям,
близким к нулю. Как показывает практика, если значение достаточно
мало, то его можно округлить или вообще обнулить без особого ущерба
для декодированного рисунка.

Этот процесс называется квантованием. И именно на этом этапе
Происходит потеря части информации. (К слову, такой же подход
используется в JPEG, только там вместо преобразования Хаара
используется дискретное косинус-преобразование. Это преобразование, на
самом деле, — тоже разновидность умножения на матрицу.) Меняя число
обнуляемых коэффициентов, можно регулировать степень сжатия!

Конечно, если обнулить слишком много, то искажения станут видны на
глаз. Во всём нужна мера!

После всех этих действий у нас останется матрица, содержащая много
нулей. Её можно записать построчно в файл и сжать каким-то
архиватором. Например, тем же 7Z. Результат будет неплох.

Декодирование производится в обратном порядке: распаковываем архив,
применяем обратное преобразование Хаара и записываем декодированную
картинку в файл. Вуаля!

Где эффективно преобразование Хаара?
====================================

Когда преобразование Хаара будет давать наилучший результат? Очевидно,
когда мы получим много нулей, то есть, когда изображение содержит
длинные участки одинаковых значений яркости. Тогда все разности
обнулятся. Это может быть, например, рентгеновский снимок,
отсканированный документ.

Говорят, что преобразование Хаара устраняет константную составляющую,
то есть переводит константы в нули. Это ещё называют устранением или
обнулением так называемых моментов нулевого порядка.

Но всё же в реальных фотографиях областей с одинаковой яркостью не так
много. Попробуем усовершенствовать преобразование, чтобы оно обнуляло
ещё и линейную составляющую. Иными словами, если значения яркости
будут увеличивать линейно, то они тоже обнулятся.

Эту задачу и более сложные (устранение моментов более высоких
порядков) решила Ингрид Добеши — один из создателей теории вейвлетов.

Преобразование Добеши
=====================

Для нашего усовершенствованного преобразования уже будет мало двух
точек. Поэтому будем брать по четыре значения, смещаясь каждый раз на
два.

То есть, если исходная последовательность имеет вид

$$1, 2, 3, 4, 5, 6, \dots, N-1, N,$$

то будем брать четвёрки с шагом 2:

$$(1, 2, 3, 4), (3, 4, 5, 6), \dots$$

Последняя четвёрка «кусает последовательность за хвост»:

$$(N-1, N, 1, 2).$$

Дело в том, что каждой четвёрке мы ставим в соответствие пару чисел.
Если в исходной последовательности всего 6 чисел, то не выходя за край
мы сможем получить только 4 числа. По ним исходные 6 не восстановить.

Чтобы выйти за край нужно сперва договориться, что за ним находится.
Можно считать, что после последнего элемента последовательность
начинается сначала (как мы только что и проделали подставив элементы
1 и 2). А можно просто подставить нули. Правильный выбор соглашения
об обработке границ влияет на качество их кодирования. Но так как
границы составляют лишь малую часть изображения, то этот вопрос
не такой уж и сильно влияет на общий принцип кодирования.

Точно так же попробуем построить два фильтра: высокочастотный и
низкочастотный. Каждую четвёрку будем заменять на два числа. Так как
четвёрки перекрываются, то количество значений после преобразования не
изменится.

Для того, чтобы было удобно считать обратную матрицу потребуем также
ортогональности преобразования. Тогда поиск обратной матрицы сведётся
к транспонированию

Пусть значения яркостей в четвёрке равны $x, y, z, t.$ Тогда первый
фильтр запишем в виде

$$a = c_1x + c_2y + c_3z + c_4z.$$

Четыре коэффициента, образующих вектор-строку матрицы преобразования,
пока нам неизвестны.

Чтобы вектор-строка коэффициентов второго фильтра был ортогонален
первому, возьмём те же коэффициенты но переставим их и поменяем знаки:

$$d = c_4x - c_3y + c_2z - c_1t.$$

Матрица преобразования будет иметь вид

$$\begin{pmatrix}
c_1 & \phantom{-}c_2 & c_3 & \phantom{-}c_4  &&& \\
c_4 &           -c_3 & c_2 &           -c_1  &&& \\
&& c_1 & \phantom{-}c_2 & c_3 & \phantom{-}c_4  & \\
&& c_4 &           -c_3 & c_2 &           -c_1  & \\
&&&&&& \ddots
\end{pmatrix}.$$

Требование ортогональности выполняется для первой и второй строк
автоматически. Потребуем, чтобы строки 1 и 3 тоже были ортогональны,
для этого их скалярное произведение должно быть равно 0:

$$c_3c_1 + c_4c_2 = 0.$$

Векторы должны иметь единичную длину (иначе определитель не будет единичным):

$$c_1^2 + c_2^2 + c_3^2 + c_4^2 = 1.$$

Преобразование должно обнулять цепочку одинаковых значений (например, $(1, 1, 1, 1)$):

$$1\cdot c_4 - 1\cdot c_3 + 1\cdot c_2 - 1\cdot c_1 = 0.$$

Преобразование должно обнулять цепочку линейно растущих значений (например, $(1, 2, 3, 4)$):

$$1\cdot c_4 - 2\cdot c_3 + 3\cdot c_2 - 4\cdot c_1 = 0.$$

Кстати, если обнуляется эта четвёрка, то будут обнуляться и любые
другие линейно растущие или линейно убывающие. В этом легко убедиться,
записав соответствующее уравнение и разделив все коэффициенты на
первый множитель.

Получили 4 уравнения, связывающие коэффициенты:
$$\begin{cases}
c_3c_1 + c_4c_2 = 0,\\
c_1^2 + c_2^2 + c_3^2 + c_4^2 = 1,\\
c_4 - c_3 + c_2 - c_1 = 0,\\
c_4 - 2c_3 + 3c_2 - 4c_1 = 0.
\end{cases}$$

Система нелинейная, и чтобы найти решение нужно немного повозиться или
воспользоваться какой-нибудь компьютерной программой.

Решение (одно из) имеет вид:

$$\begin{cases}
c_1 = \frac{1+\sqrt3}{4\sqrt2},\\
c_2 = \frac{3+\sqrt3}{4\sqrt2},\\
c_3 = \frac{3-\sqrt3}{4\sqrt2},\\
c_4 = \frac{1-\sqrt3}{4\sqrt2}.\\
\end{cases}$$

Подставив коэффициенты в матрицу, получаем искомое
преобразования. После его применения к фотографиям получим больше
нулей и малых коэффициентов, чем после преобразования Хаара, что
позволит сжать изображение сильнее.

Другая приятная особенность — артефакты после квантования будут не так
заметны.

Это преобразование получило название вейвлета D4 (читателю
предлагается самостоятельно разгадать тайну этого буквенно-цифрового
названия).

О нормировке
============

Если посмотреть коэффициенты в
[википедии](http://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%B9%D0%B2%D0%BB%D0%B5%D1%82%D1%8B_%D0%94%D0%BE%D0%B1%D0%B5%D1%88%D0%B8),
то можно заметить, что они отличаются от тех, что получили мы. Дело в
том, что в теории вейвлетов обычно используется нормировка не на 1, а
на 2. То есть, произведение матрицы преобразования и матрицы обратного
преобразования должно быть равно удвоенной единичной матрице, а не просто единичной.

Если заменить в правой части второго уравнения 1 на 2 и решить систему,
то получим те же коэффициенты, что и в Википедии.

А, скажем, в статье P. Getreuer [Filter Coefficients to Popular
Wavelets](http://read.pudn.com/downloads129/sourcecode/math/551883/Filter%20Coefficients%20to%20Popular%20Wavelets.PDF),
в которой приводятся коэффициенты для распространённых вейвлетов,
значения те же, что и у нас.

Сравнение вейвлетов Хаара и Добеши
==================================

Как и в случае вейвлета Хаара, вейвлет D4 можно применить к
цветному изображению.

![Двумерное преобразование]({{ site.url }}/assets/2019-01-31-wavelets-theory/lena-daubechies-1.jpg)

Мы, конечно, можем не остановиться на этом, и потребовать устранения
параболической составляющей (момент 2-го порядка) и так далее. В
результате получим вейвлеты D6, D8 и другие.

Для наглядности сравним вейвлеты Хаара (слева) и Добеши D8 (справа).

![Двумерное преобразование]({{ site.url }}/assets/2019-01-31-wavelets-theory/lena-haar-daubechies.jpg)

Если приглядеться, то видно, что после преобразования Добеши в
высокочастотной области меньше деталей. Например, нет вертикальных
линий, соответствующим контрастным переходам на фоне. Это значит, что
значения в этих областях ближе к нулю и их проще сжать.

Добеши предложила весьма интересный способ получения коэффициентов этих
преобразований, но увы, это уже выходит за рамки нашей статьи.
Мы не пытаемся построить теорию вейвлетов, а лишь выводим их из
простых практических соображений.

Биортогональные вейвлеты
========================

Разумеется, изложенные соображения могут быть различными в зависимости
от задачи. Например, для простоты выполнения обратного преобразования
мы потребовали, чтобы соответствующая матрица была ортогональной.
Это, конечно, удобно, но коэффициенты такого преобразования
несимметричны. Может, это слишком строгое требование?

И действительно, можно для кодирования использовать неортогональную
матрицу, но не произвольную, а то придётся считать обратную к ней. Мы
постараемся выберать коэффициенты так, чтобы часть требований к
обратной матрице выполнилась автоматически. Заодно сделаем
коэффициенты фильтров высоких частот и фильтра низких частот
симметричными.

Рассмотрим следующее интересное вейвлет-преобразование, которое
называется CDF 5/3. Он назван так по первым буквам фамилий — Коэн,
Добеши и Фово. А цифры — это длины фильтров.

$$H = \begin{pmatrix}
   0 &  a_1 &  a_0 &  a_1 &    0 &    0 &      &      & \\
   0 &  b_2 &  b_1 &  b_0 &  b_1 &  b_2 &      &      & \\
     &      &    0 &  a_1 &  a_0 &  a_1 &    0 &    0 & \\
     &      &    0 &  b_2 &  b_1 &  b_0 &  b_1 &  b_2 & \\
     &      &      &      &      &      &      &      & \ddots
\end{pmatrix}.$$

Обратите внимание, что хотя длины фильтров (нули мы, конечно, в расчёт
не берём) равны 3 и 5, сами фильтры симметричны и нам нужно найти лишь
5 различных чисел.

Обратное преобразование мы построим следующим хитрым образом из тех же
самых чисел просто переставив их и поменяв знаки.

$$\bar{H} = \begin{pmatrix}
\phantom{-}b_2 &              0 &                &                & \\
          -b_1 &              0 &                &                & \\
\phantom{-}b_0 &           -a_1 & \phantom{-}b_2 &              0 & \\
          -b_1 & \phantom{-}a_0 &           -b_1 &              0 & \\
\phantom{-}b_2 &           -a_1 & \phantom{-}b_0 &           -a_1 & \\
               &                &           -b_1 & \phantom{-}a_0 & \\
               &                & \phantom{-}b_2 &           -a_1 & \\
               &                &                &                & \ddots
\end{pmatrix}.$$

Такое странное на первый взгляд расположение обладает важными
преимуществами. Во-первых для того, чтобы задать и матрицу
преобразования, и обратную матрицу нужно лишь найти пять чисел, то
есть мы должны записать систему из пяти уравнений. А во-вторых, при
перемножении этих матриц некоторые произведения строк на столбцы сразу
равны нулю. Это значит, что мы можем сэкономить пару строк в системе
уравнений для каких-то полезных ограничений.

Давайте посмотрим, какие ограничения нам нужны, чтобы вторая матрица
была обратной к первой, то есть

$$
H \bar{H} = E =
\begin{pmatrix}
1 & 0 & \\
0 & 1 & \\
  &   & \ddots
\end{pmatrix}.
$$

Скалярное произведение первой строки $$H$$ на первый стобец
$$\bar{H}$$, второй строки на второй столбец и так далее должны быть
равны 1:

$$0 \cdot b_2 - a_1 \cdot b_1 + a_0 \cdot b_0 - a_1 \cdot b_1 + 0 \cdot b_2 = 1,$$

$$b_2 \cdot 0 - b_1 \cdot a_1 + b_0 \cdot a_0 - b_1 \cdot a_1 + b_2 \cdot 0 = 1.$$

Заметьте, что это на самом деле одно и то же уравнение

$$a_0 b_0 - 2 a_1 b_1 = 1.$$

Для остальных строк и столбцов с одинаковыми номерами уравнения будут
такими же.

Остальные произведения строк и столбцов должны быть равны 0:

$$a_1 \cdot 0 - a_0 \cdot a_1 - a_1 \cdot a_0 - 0 \cdot a_1 \equiv 0,$$

$$a_1 \cdot 0 + a_0 \cdot b_2 - a_1 \cdot b_1 + 0 \cdot b_0 - 0 \cdot b_1 + 0 \cdot b_2 = 0,$$

$$0 \cdot b_2 - b_2 \cdot b_1 + b_1 \cdot b_0 - b_0 \cdot b_1 + b_1 \cdot b_2 + b_2 \cdot 0 \equiv 0,$$

$$b_2 \cdot 0 + b_1 \cdot 0 + b_0 \cdot 0 - b_1 \cdot a_1 + b_2 \cdot a_0 - 0 \cdot a_1 = 0.$$

Если отбросить тождества, то получаем два эквивалентных уравнения, то
есть, фактически, лишь одно новое ограничение:

$$a_0 b_2 - a_1 b_1 = 0.$$

Итак, у нас уже два уравнения и пять неизвестных. То есть ещё три
уравнения мы можем выбрать на своё усмотрение.

Потребуем, чтобы фильтр высоких частот преобразования $$H$$
(то есть, пропускающий высокие частоты) обнулял моменты нулевого и
первого порядков:

$$1 \cdot b_2 + 1 \cdot b_1 + 1 \cdot b_0 + 1 \cdot b_1 + 1 \cdot b_2 = 0,$$

$$1 \cdot b_2 + 2 \cdot b_1 + 3 \cdot b_0 + 4 \cdot b_1 + 5 \cdot b_2 = 0.$$

Легко убедиться, что это одно и то же уравнение:

$$b_0 + 2 b_1 + 2 b_2 = 0.$$

Того же потребуем и для преобразования $$\bar{H}$$ (его называют дуальным к $$H$$):

$$1 \cdot (-a_1) + 2 \cdot a_0 + 3 \cdot (-a_1) = 0,$$

то есть

$$a_0 - 2 a_1 = 0.$$

У нас осталось в запасе одно требование, но попробуем решить без него.
Получаем систему уравнений:

$$\begin{cases}
a_0 b_0 - 2 a_1 b_1 = 1, \\
a_0 b_2 - a_1 b_1 = 0, \\
b_0 + 2 b_1 + 2 b_2 = 0, \\
a_0 - 2 a_1 = 0.
\end{cases}$$

Решение этой системы:

$$\begin{cases}
a_1 = \phantom{-}\frac{a_0}2, \\
b_0 = \phantom{-}\frac3{4 a_0}, \\
b_1 = -\frac1{4 a_0}, \\
b_2 = -\frac1{8 a_0}.
\end{cases}$$

То есть, пятое уравнение, если бы мы его добавили, повлияло бы только
лишь на нормировку. Ведь при применении преобразования все операции
линейные. Значит $$a_0$$ для одного фильтра и $$\frac1{a_0}$$ для
другого можно просто вынести за скобку.

Получившееся преобразование благодаря своим интересным свойствам нашло
применение в алгоритме сжатия изображений JPEG2000.

Если поискать коэффициенты для CDF 5/3, то в разных источниках
нормировка разная. Возьмём для определённости $$a_0=1$$ как в JPEG2000.

Тогда получаем:

$$\begin{cases}
a_0 = \phantom{-}1, \\
a_1 = \phantom{-}\frac12, \\
b_0 = \phantom{-}\frac34, \\
b_1 = -\frac14, \\
b_2 = -\frac18.
\end{cases}$$

Обратите внимание, что все коэффициенты кроме $$a_0$$ — это дроби, у
которых в знаменателе степень двойки, а значит вычисления с такими
коэффициентами можно очень эффективно реализовать на компьютере в
двоичной системе.

Домашнее задание
================

Чтобы окончательно разобраться с основами, предлагаю написать на вашем
любимом языке программу, которая открывает изображение, выполняет
преобразование Хаара (или даже D4), квантует результат, а потом
сохраняет результат в файл. Попробуйте сжать этот файл своим любимым
архиватором. Хорошо сжимается?

Попробуйте выполнить обратное преобразование. Как вы объясните
характер артефактов на изображении?

Заключение
==========

Итак, мы кратко рассмотрели основные идеи дискретного
вейвлет-преобразования.

Конечно, в этой статье не были рассмотрены очень многие интересные
математические детали и практические применения
вейвлет-преобразований. На практике применяются более сложные, но и
более эффективные подходы, такие как
[лифтинг-схема](https://ru.wikipedia.org/wiki/%D0%A1%D1%85%D0%B5%D0%BC%D0%B0_%D0%BB%D0%B8%D1%84%D1%82%D0%B8%D0%BD%D0%B3%D0%B0).
Разумеется, это только самые основы. И на самом деле вейвлеты
конструируются не так, как описано. Но многое сложно объяснить не
повышая градус матана. Надеюсь, что у читателя после прочтения этой
статьи появилось общее представление о том, как используются вейвлеты
в области сжатия изображений.

Литература
==========

Есть много довольно неплохих книжек, которые дают более глубокое
представление о вейвлетах. Начать рекомендую со следующих:

- Уэлстид С. Фракталы и вейвлеты для сжатия изображений в действии. — М.: Триумф, 2003.
- Штарк Г.-Г. Применение вейвлетов для ЦОС. — М.: Техносфера, 2007.
- Usevitch B. [A tutorial on modern lossy wavelet image compression: Foundations of JPEG 2000.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.7887&rep=rep1&type=pdf) Signal Processing Magazine, IEEE. 18. 22 - 35. DOI: 10.1109/79.952803.
